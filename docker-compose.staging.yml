services:
  backend:
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    env_file:
      - config/environments/staging.env
    volumes: []
    ports: []

  worker:
    env_file:
      - config/environments/staging.env
    volumes: []

  flower:
    env_file:
      - config/environments/staging.env
    volumes: []
    ports: []

  ml-inference:
    env_file:
      - config/environments/staging.env
    volumes: []
    ports: []

  frontend:
    env_file:
      - config/environments/staging.env
    command: ["npm", "run", "preview", "--", "--host", "0.0.0.0", "--port", "4173"]
    volumes: []
    ports: []

  reverse-proxy:
    image: caddy:2.7
    ports:
      - "80:80"
      - "443:443"
    environment:
      LETSENCRYPT_EMAIL: ${LETSENCRYPT_EMAIL:-admin@example.com}
      APP_HOST: ${APP_HOST:-staging.socializer.test}
      API_HOST: ${API_HOST:-api.staging.socializer.test}
      FLOWER_HOST: ${FLOWER_HOST:-flower.staging.socializer.test}
      ML_HOST: ${ML_HOST:-ml.staging.socializer.test}
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      backend:
        condition: service_started
      frontend:
        condition: service_started
      flower:
        condition: service_started
      ml-inference:
        condition: service_started

volumes:
  caddy_data:
  caddy_config:
